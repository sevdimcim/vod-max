import cloudscraper
import json
import re
import os
import time
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor

# --- AYARLAR ---
BASE_URL = "https://dizipal.cx"
MAX_WORKERS = 15
OUTPUT_FOLDER = "atom"

# Platform Listesi (URL slug : Dosya AdÄ±)
# TV'yi turkcelltv olarak kaydetmek istediÄŸin iÃ§in mapping yaptÄ±k.
PLATFORMS = {
    "netflix": "netflix",
    "exxen": "exxen",
    "prime-video": "prime-video",
    "tabii": "tabii",
    "apple-tv": "apple-tv",
    "disney": "disney",
    "hbomax": "hbomax",
    "gain": "gain",
    "mubi": "mubi",
    "tod": "tod",
    "hulu": "hulu",
    "tv": "turkcelltv"  # URL'de 'tv', dosyada 'turkcelltv' olacak
}

# TarayÄ±cÄ± simÃ¼lasyonu
scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
)

def get_source(url):
    try:
        # Sayfa geÃ§iÅŸlerinde Ã§ok seri istek atÄ±p ban yememek iÃ§in minik bekleme
        time.sleep(0.5) 
        res = scraper.get(url, timeout=10)
        return res.text if res.status_code == 200 else None
    except:
        return None

def get_highest_res_image(srcset_content):
    """srcset iÃ§indeki en yÃ¼ksek kaliteli resim linkini ayÄ±klar."""
    if not srcset_content:
        return ""
    parts = [s.strip().split(' ')[0] for s in srcset_content.split(',')]
    return parts[-1] if parts else ""

def fetch_iframe_only(ep_url):
    html = get_source(ep_url)
    if html:
        iframe = re.search(r'<iframe[^>]+src=["\']([^"\']+)["\']', html)
        if iframe:
            return iframe.group(1)
    return None

def clean_key(text):
    text = re.sub(r'[\s\:\,\']+', '-', text)
    text = re.sub(r'-+', '-', text)
    return text.strip('-')

def scrape_platform(slug, filename_base):
    """Tek bir platformu baÅŸtan sona tarar ve kaydeder."""
    
    json_path = os.path.join(OUTPUT_FOLDER, f"{filename_base}.json")
    
    # EÄŸer dosya varsa Ã¼zerine yazmak yerine mevcut veriyi okuyup devam edebilirsin
    # Åimdilik sÄ±fÄ±rdan baÅŸlatalÄ±m ki temiz olsun:
    results = {} 
    
    print(f"\nğŸŒ PLATFORM TARANIYOR: {slug.upper()} -> {filename_base}.json")
    
    page_num = 1
    found_any_on_platform = False

    while True:
        page_url = f"{BASE_URL}/platform/{slug}/page/{page_num}/"
        print(f"   ğŸ“„ Sayfa {page_num} kontrol ediliyor...")
        
        html = get_source(page_url)
        
        # Sayfa boÅŸsa veya iÃ§erik yoksa dÃ¶ngÃ¼yÃ¼ kÄ±r (Otomatik Sayfa AlgÄ±lama)
        if not html:
            print(f"   â›” Sayfa {page_num} yÃ¼klenemedi, platform tamamlandÄ± sanÄ±rÄ±m.")
            break
        
        # Ä°Ã§erik var mÄ± kontrolÃ¼ (post-item class'Ä± var mÄ±?)
        items = re.findall(r'<div class="post-item">.*?href="(.*?)".*?title="(.*?)".*?data-srcset="(.*?)"', html, re.S)
        
        if not items:
            print(f"   ğŸš« Sayfa {page_num} iÃ§inde iÃ§erik bulunamadÄ±. Platform sonu.")
            break
            
        found_any_on_platform = True
        
        # Bu sayfadaki iÃ§erikleri iÅŸle
        for link, title, srcset in items:
            original_name = title
            json_key = clean_key(original_name)
            
            # EÄŸer zaten eklediysek atla
            if json_key in results:
                continue
                
            poster_url = get_highest_res_image(srcset)
            main_link = urljoin(BASE_URL, link)
            
            print(f"      ğŸ’ Ä°ÅŸleniyor: {original_name}")
            
            results[json_key] = {
                "isim": original_name,
                "resim": poster_url,
                "bolumler": []
            }
            
            # Ä°Ã§erik detayÄ±na git
            source = get_source(main_link)
            if not source: continue

            # Sezon ve BÃ¶lÃ¼m Toplama
            seasons = re.findall(r'href=["\']([^"\']+\?sezon=\d+)["\']', source)
            season_urls = sorted(list(set([urljoin(BASE_URL, s) for s in seasons])))
            
            if not season_urls: 
                season_urls = [main_link]
            else:
                if main_link not in season_urls: 
                    season_urls.insert(0, main_link)

            all_ep_links = []
            for s_url in season_urls:
                s_html = get_source(s_url)
                if s_html:
                    eps = re.findall(r'href=["\']([^"\']+(?:bolum|anime-bolum)/[^"\']+)["\']', s_html)
                    # BÃ¶lÃ¼m sÄ±ralamasÄ±
                    eps = sorted(list(set(eps)), key=lambda x: [int(c) if c.isdigit() else c for c in re.split(r'(\d+)', x)])
                    for e in eps:
                        full_e = urljoin(BASE_URL, e)
                        if full_e not in all_ep_links:
                            all_ep_links.append(full_e)

            if all_ep_links:
                # Thread ile hÄ±zlÄ±ca iframe Ã§ek
                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    iframe_list = list(executor.map(fetch_iframe_only, all_ep_links))
                
                count = 1
                for iframe_link in iframe_list:
                    if iframe_link:
                        results[json_key]["bolumler"].append({
                            "bolum_baslik": f"{original_name} {count}. BÃ¶lÃ¼m",
                            "link": iframe_link
                        })
                        count += 1
            else:
                # Film durumu
                iframe = re.search(r'<iframe[^>]+src=["\']([^"\']+)["\']', source)
                if iframe:
                    results[json_key]["bolumler"].append({
                        "bolum_baslik": f"{original_name} (Film)",
                        "link": iframe.group(1)
                    })

            # Her iÃ§erik eklendiÄŸinde dosyayÄ± gÃ¼ncelle (Crash olursa veri kaybÄ± olmasÄ±n)
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
        
        # Bir sonraki sayfaya geÃ§
        page_num += 1

def main():
    # 1. KlasÃ¶r oluÅŸtur
    if not os.path.exists(OUTPUT_FOLDER):
        os.makedirs(OUTPUT_FOLDER)
        print(f"ğŸ“ '{OUTPUT_FOLDER}' klasÃ¶rÃ¼ oluÅŸturuldu.")

    # 2. Her platformu sÄ±rayla tara
    for slug, filename in PLATFORMS.items():
        try:
            scrape_platform(slug, filename)
        except Exception as e:
            print(f"âŒ {slug} platformunda hata oluÅŸtu: {str(e)}")
            continue

    print("\nğŸ TÃœM Ä°ÅLEMLER TAMAMLANDI! ğŸ")

if __name__ == "__main__":
    main()
